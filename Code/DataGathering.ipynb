{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import h3\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DeepAnnualUrbanHarvester:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the harvester with API endpoint and directory structures.\n",
    "        \"\"\"\n",
    "        self.api_url = \"http://overpass-api.de/api/interpreter\"\n",
    "        self.home_dir = os.path.expanduser(\"~\")\n",
    "        \n",
    "        # Directory configuration\n",
    "        self.task_list_dir = os.path.join(self.home_dir, \"Desktop\", \"Infor 301\", \"final\", \"task_lists\")\n",
    "        self.output_base_dir = os.path.join(self.home_dir, \"Desktop\", \"Infor 301\", \"final\", \"Annual_POI_Data\")\n",
    "        \n",
    "        if not os.path.exists(self.output_base_dir):\n",
    "            os.makedirs(self.output_base_dir)\n",
    "\n",
    "    def fetch_osm_historical_data(self, bbox, year):\n",
    "        \"\"\"\n",
    "        Fetch historical POI data from Overpass API for a specific year and bounding box.\n",
    "        \"\"\"\n",
    "        timestamp = f\"{year}-07-01T00:00:00Z\"\n",
    "        # Querying common socio-economic POI tags\n",
    "        query = f\"\"\"[out:json][timeout:3600][date:\"{timestamp}\"];\n",
    "        (\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"amenity\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"shop\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"leisure\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"office\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"tourism\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"craft\"];\n",
    "        );\n",
    "        out tags center;\"\"\"\n",
    "        \n",
    "        headers = {'User-Agent': 'Urban_SocioEconomic_Harvester_v1.0'}\n",
    "        \n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(self.api_url, data={'data': query}, headers=headers, timeout=3700)\n",
    "                if response.status_code == 200:\n",
    "                    elements = response.json().get('elements', [])\n",
    "                    if elements:\n",
    "                        return elements\n",
    "                    else:\n",
    "                        wait_time = (attempt + 1) * 10\n",
    "                        print(f\"  [Warning] Year {year}: 0 results. Retrying in {wait_time}s... ({attempt+1}/{max_retries})\")\n",
    "                        time.sleep(wait_time)\n",
    "                elif response.status_code == 429:\n",
    "                    print(\"  [Limit] Rate limit reached (429). Sleeping for 60s...\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"  [Error] HTTP {response.status_code}. Retrying...\")\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f\"  [Exception] Network error: {e}. Retrying...\")\n",
    "                time.sleep(10)\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def classify_tags(self, tags):\n",
    "        \"\"\"\n",
    "        Identify categories based on POI tags. Returns a list of matched categories.\n",
    "        \"\"\"\n",
    "        tag_values_flattened = \" \".join(str(v).lower() for v in tags.values())\n",
    "        \n",
    "        category_keywords = {\n",
    "            \"Finance_Wealth\": ['bank', 'atm', 'exchange', 'investment', 'chase', 'hsbc', 'barclays', 'goldman', 'morgan', 'jewelry', 'rolex'],\n",
    "            \"High_End_Consumption\": ['boutique', 'luxury', 'michelin', 'fine_dining', 'gucci', 'prada', 'hermes', 'chanel', 'dior', 'louis vuitton'],\n",
    "            \"Art_Culture_Tourism\": ['museum', 'gallery', 'theatre', 'arts_centre', 'hotel', 'hilton', 'marriott', 'hyatt', 'attraction'],\n",
    "            \"Social_Resilience\": ['hospital', 'university', 'college', 'school', 'library', 'clinic', 'pharmacy'],\n",
    "            \"Commercial_Vitality\": ['office', 'coworking', 'commercial', 'tower', 'plaza', 'center', 'consulting'],\n",
    "            \"Mass_Retail\": ['supermarket', 'mall', 'department_store', 'walmart', 'target', 'tesco', 'costco', 'convenience'],\n",
    "            \"Night_Economy\": ['pub', 'bar', 'nightclub', 'speakeasy', 'casino']\n",
    "        }\n",
    "        \n",
    "        matches = []\n",
    "        for category, keywords in category_keywords.items():\n",
    "            if any(keyword in tag_values_flattened for keyword in keywords):\n",
    "                matches.append(category)\n",
    "        return matches\n",
    "\n",
    "    def process_all_tasks(self, start_year=2016, end_year=2025):\n",
    "        \"\"\"\n",
    "        Iterate through task files and fetch data for each year.\n",
    "        \"\"\"\n",
    "        task_files = [f for f in os.listdir(self.task_list_dir) if f.endswith(\".csv\")]\n",
    "        \n",
    "        for file_name in task_files:\n",
    "            print(f\"\\n>>> Starting Task: {file_name}\")\n",
    "            task_path = os.path.join(self.task_list_dir, file_name)\n",
    "            grid_df = pd.read_csv(task_path)\n",
    "            \n",
    "            # Define dynamic bounding box\n",
    "            bbox = [\n",
    "                grid_df['latitude'].min(), \n",
    "                grid_df['longitude'].min(), \n",
    "                grid_df['latitude'].max(), \n",
    "                grid_df['longitude'].max()\n",
    "            ]\n",
    "            \n",
    "            city_label = file_name.replace(\"_Res8_task_list.csv\", \"\")\n",
    "            city_dir = os.path.join(self.output_base_dir, city_label)\n",
    "            if not os.path.exists(city_dir): \n",
    "                os.makedirs(city_dir)\n",
    "\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                output_file = os.path.join(city_dir, f\"{city_label}_{year}.csv\")\n",
    "                \n",
    "                # Skip if already processed\n",
    "                if os.path.exists(output_file):\n",
    "                    print(f\"  [Skip] {city_label} {year} already exists.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"  [Running] Year: {year} | Fetching POI data...\")\n",
    "                raw_data = self.fetch_osm_historical_data(bbox, year)\n",
    "                \n",
    "                if not raw_data:\n",
    "                    print(f\"  [Failure] No data retrieved for {year}.\")\n",
    "                    continue\n",
    "\n",
    "                # Mapping and Counting Logic\n",
    "                h3_counts = {}\n",
    "                metric_cols = [\n",
    "                    \"Finance_Wealth\", \"High_End_Consumption\", \"Art_Culture_Tourism\",\n",
    "                    \"Social_Resilience\", \"Commercial_Vitality\", \"Mass_Retail\", \"Night_Economy\"\n",
    "                ]\n",
    "\n",
    "                for item in raw_data:\n",
    "                    lat = item.get('lat') or item.get('center', {}).get('lat')\n",
    "                    lon = item.get('lon') or item.get('center', {}).get('lon')\n",
    "                    if not lat or not lon: continue\n",
    "                    \n",
    "                    cell_id = h3.latlng_to_cell(lat, lon, 8)\n",
    "                    tags = item.get('tags', {})\n",
    "                    \n",
    "                    if cell_id not in h3_counts:\n",
    "                        h3_counts[cell_id] = {col: 0 for col in metric_cols}\n",
    "                    \n",
    "                    matched_categories = self.classify_tags(tags)\n",
    "                    for cat in matched_categories:\n",
    "                        h3_counts[cell_id][cat] += 1\n",
    "\n",
    "                # Merge with original grid structure\n",
    "                yearly_stats_df = pd.DataFrame.from_dict(h3_counts, orient='index').reset_index()\n",
    "                yearly_stats_df.rename(columns={'index': 'h3_index'}, inplace=True)\n",
    "                \n",
    "                final_df = grid_df.merge(yearly_stats_df, on='h3_index', how='left').fillna(0)\n",
    "                final_df.to_csv(output_file, index=False)\n",
    "                \n",
    "                print(f\"  [Success] Saved {year} | Cells with activity: {len(yearly_stats_df)}\")\n",
    "                \n",
    "                # Mandatory 3-second delay as requested\n",
    "                time.sleep(3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    harvester = DeepAnnualUrbanHarvester()\n",
    "    harvester.process_all_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import h3\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Ultimate_Urban_Evolution_Harvester:\n",
    "    def __init__(self):\n",
    "        # Overpass API endpoint\n",
    "        self.api_url = \"http://overpass-api.de/api/interpreter\"\n",
    "        self.home = os.path.expanduser(\"~\")\n",
    "        \n",
    "        # Directory configuration\n",
    "        self.task_list_dir = os.path.join(self.home, \"Desktop\", \"Infor 301\", \"final\", \"task_lists\")\n",
    "        self.output_base_dir = os.path.join(self.home, \"Desktop\", \"Infor 301\", \"final\", \"Annual_POI_Data\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(self.output_base_dir):\n",
    "            os.makedirs(self.output_base_dir)\n",
    "\n",
    "    def fetch_chunk_data(self, bbox, year):\n",
    "        \"\"\"\n",
    "        Fetches historical OSM data using the [date:...] meta-tag.\n",
    "        Optimized for small spatial chunks to prevent API timeouts.\n",
    "        \"\"\"\n",
    "        timestamp = f\"{year}-07-01T00:00:00Z\"\n",
    "        \n",
    "        # Query for all core socio-economic nodes\n",
    "        query = f\"\"\"[out:json][timeout:1800][date:\"{timestamp}\"];\n",
    "        (\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"amenity\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"shop\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"leisure\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"office\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"tourism\"];\n",
    "          node({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]})[\"craft\"];\n",
    "        );\n",
    "        out tags center;\"\"\"\n",
    "        \n",
    "        headers = {'User-Agent': 'Urban_Dynamics_Research_V9'}\n",
    "        \n",
    "        # Retry mechanism (3 attempts)\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = requests.post(self.api_url, data={'data': query}, headers=headers, timeout=1900)\n",
    "                if response.status_code == 200:\n",
    "                    return response.json().get('elements', [])\n",
    "                elif response.status_code == 429:\n",
    "                    # Rate limited: wait longer\n",
    "                    time.sleep(60) \n",
    "            except:\n",
    "                time.sleep(20)\n",
    "        return []\n",
    "\n",
    "    def classify_element(self, tags):\n",
    "        \"\"\"\n",
    "        Keyword-Enhanced Taxonomy: Captures socio-economic status hidden in brand names.\n",
    "        \"\"\"\n",
    "        t = str(tags).lower()\n",
    "        \n",
    "        # Define academic dimensions with expanded keywords\n",
    "        keywords = {\n",
    "            \"Finance_Wealth\": ['bank', 'atm', 'exchange', 'investment', 'chase', 'hsbc', 'barclays', 'goldman', 'morgan', 'jewelry', 'rolex'],\n",
    "            \"High_End_Consumption\": ['boutique', 'luxury', 'michelin', 'fine_dining', 'gucci', 'prada', 'hermes', 'chanel', 'dior', 'louis vuitton'],\n",
    "            \"Art_Culture_Tourism\": ['museum', 'gallery', 'theatre', 'arts_centre', 'hotel', 'hilton', 'marriott', 'hyatt', 'attraction'],\n",
    "            \"Social_Resilience\": ['hospital', 'university', 'college', 'school', 'library', 'clinic', 'pharmacy'],\n",
    "            \"Commercial_Vitality\": ['office', 'coworking', 'commercial', 'tower', 'plaza', 'center', 'consulting'],\n",
    "            \"Mass_Retail\": ['supermarket', 'mall', 'department_store', 'walmart', 'target', 'tesco', 'costco', 'convenience'],\n",
    "            \"Night_Economy\": ['pub', 'bar', 'nightclub', 'speakeasy', 'casino']\n",
    "        }\n",
    "        \n",
    "        return {category: any(x in t for x in keys) for category, keys in keywords.items()}\n",
    "\n",
    "    def run_annual_pipeline(self, start_year=2016, end_year=2025):\n",
    "        # Scan for task list CSVs generated by your boundary script\n",
    "        task_files = [f for f in os.listdir(self.task_list_dir) if f.endswith(\".csv\")]\n",
    "        \n",
    "        for file_name in task_files:\n",
    "            city_name = file_name.replace(\"_Res8_task_list.csv\", \"\")\n",
    "            print(f\"\\nðŸŒ --- Initializing Deep Collection for: {city_name} ---\")\n",
    "            \n",
    "            task_path = os.path.join(self.task_list_dir, file_name)\n",
    "            grid_df = pd.read_csv(task_path)\n",
    "            \n",
    "            # Dynamically calculate the Bounding Box from your task list data\n",
    "            min_lat, max_lat = grid_df['latitude'].min(), grid_df['latitude'].max()\n",
    "            min_lon, max_lon = grid_df['longitude'].min(), grid_df['longitude'].max()\n",
    "\n",
    "            # Partition the city into a 4x4 grid (16 chunks) for data integrity\n",
    "            lat_grid = np.linspace(min_lat, max_lat, 5)\n",
    "            lon_grid = np.linspace(min_lon, max_lon, 5)\n",
    "\n",
    "            city_output_dir = os.path.join(self.output_base_dir, city_name)\n",
    "            if not os.path.exists(city_output_dir): os.makedirs(city_output_dir)\n",
    "\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                save_path = os.path.join(city_output_dir, f\"{city_name}_{year}.csv\")\n",
    "                \n",
    "                # Skip if data for the year already exists (Breakpoint resume)\n",
    "                if os.path.exists(save_path):\n",
    "                    print(f\"â­ï¸  Year {year} exists, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"ðŸ“… Harvesting {year} data (Iterating through 16 spatial chunks)...\")\n",
    "                annual_h3_data = {}\n",
    "\n",
    "                # Execute spatial chunk loop\n",
    "                for i in range(4):\n",
    "                    for j in range(4):\n",
    "                        sub_bbox = [lat_grid[i], lon_grid[j], lat_grid[i+1], lon_grid[j+1]]\n",
    "                        elements = self.fetch_chunk_data(sub_bbox, year)\n",
    "                        \n",
    "                        for el in elements:\n",
    "                            lat = el.get('lat') or el.get('center', {}).get('lat')\n",
    "                            lon = el.get('lon') or el.get('center', {}).get('lon')\n",
    "                            if not lat or not lon: continue\n",
    "                            \n",
    "                            # Map to H3 Cell (Resolution 8)\n",
    "                            hid = h3.latlng_to_cell(lat, lon, 8)\n",
    "                            if hid not in annual_h3_data:\n",
    "                                annual_h3_data[hid] = {k: 0 for k in [\n",
    "                                    \"Finance_Wealth\", \"High_End_Consumption\", \"Art_Culture_Tourism\",\n",
    "                                    \"Social_Resilience\", \"Commercial_Vitality\", \"Mass_Retail\", \"Night_Economy\"\n",
    "                                ]}\n",
    "                            \n",
    "                            # Semantic classification\n",
    "                            categories = self.classify_element(el.get('tags', {}))\n",
    "                            for cat, val in categories.items():\n",
    "                                if val: annual_h3_data[hid][cat] += 1\n",
    "                        \n",
    "                        # Respect the 15-second cooling down interval per chunk\n",
    "                        time.sleep(15)\n",
    "\n",
    "                # Map results back to your original H3 grid structure\n",
    "                annual_df = pd.DataFrame.from_dict(annual_h3_data, orient='index').reset_index()\n",
    "                annual_df.rename(columns={'index': 'h3_index'}, inplace=True)\n",
    "                \n",
    "                # Left merge ensures every cell in your task list is represented\n",
    "                final_output = grid_df.merge(annual_df, on='h3_index', how='left').fillna(0)\n",
    "                final_output.to_csv(save_path, index=False)\n",
    "                print(f\"âœ… Success: {city_name} {year} | Active Cells: {len(annual_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Ultimate_Urban_Evolution_Harvester().run_annual_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class UrbanDataOptimizer:\n",
    "    def __init__(self):\n",
    "        self.home_dir = os.path.expanduser(\"~\")\n",
    "        self.base_path = os.path.join(self.home_dir, \"Desktop\", \"Infor 301\", \"final\")\n",
    "        self.osm_dir = os.path.join(self.base_path, \"Annual_POI_Data\")\n",
    "        self.task_dir = os.path.join(self.base_path, \"task_lists\")\n",
    "        # ä¿®æ”¹ç‚¹ 1: ä¿®æ”¹è¾“å‡ºæ–‡ä»¶å¤¹åç§°\n",
    "        self.filtered_task_dir = os.path.join(self.base_path, \"cleaned_POI_2025\")\n",
    "        \n",
    "        if not os.path.exists(self.filtered_task_dir):\n",
    "            os.makedirs(self.filtered_task_dir)\n",
    "\n",
    "    def filter_active_grids(self, strict_2025=True):\n",
    "        \"\"\"\n",
    "        strict_2025=True: ä»…æ ¹æ® 2025 å¹´æ•°æ®è¿‡æ»¤\n",
    "        strict_2025=False: æ ¹æ® 2016-2025 æ‰€æœ‰åŽ†å²è®°å½•è¿‡æ»¤ (æŽ¨èï¼Œé˜²æ­¢é—æ¼æ¼”åŒ–ç‚¹)\n",
    "        \"\"\"\n",
    "        cities = [d for d in os.listdir(self.osm_dir) if os.path.isdir(os.path.join(self.osm_dir, d))]\n",
    "        poi_columns = [\"Finance_Wealth\", \"High_End_Consumption\", \"Art_Culture_Tourism\",\n",
    "                       \"Social_Resilience\", \"Commercial_Vitality\", \"Mass_Retail\", \"Night_Economy\"]\n",
    "        report = []\n",
    "\n",
    "        for city in tqdm(cities, desc=\"Filtering Active Grids\"):\n",
    "            city_path = os.path.join(self.osm_dir, city)\n",
    "            \n",
    "            # ç­›é€‰æ–‡ä»¶\n",
    "            if strict_2025:\n",
    "                osm_files = [f for f in os.listdir(city_path) if \"2025\" in f and f.endswith(\".csv\")]\n",
    "            else:\n",
    "                osm_files = [f for f in os.listdir(city_path) if f.endswith(\".csv\")]\n",
    "            \n",
    "            active_h3_indices = set()\n",
    "            for f in osm_files:\n",
    "                df_temp = pd.read_csv(os.path.join(city_path, f))\n",
    "                active_mask = df_temp[poi_columns].sum(axis=1) > 0\n",
    "                active_h3_indices.update(df_temp.loc[active_mask, 'h3_index'].tolist())\n",
    "            \n",
    "            original_task_path = os.path.join(self.task_dir, f\"{city}_Res8_task_list.csv\")\n",
    "            if os.path.exists(original_task_path):\n",
    "                df_task = pd.read_csv(original_task_path)\n",
    "                df_filtered = df_task[df_task['h3_index'].isin(active_h3_indices)].copy()\n",
    "                \n",
    "                # ä¿®æ”¹ç‚¹ 2: ä¿®æ”¹è¾“å‡ºæ–‡ä»¶å\n",
    "                out_path = os.path.join(self.filtered_task_dir, f\"{city}_cleaned_2025.csv\")\n",
    "                df_filtered.to_csv(out_path, index=False)\n",
    "                \n",
    "                reduction = (1 - len(df_filtered) / len(df_task)) * 100\n",
    "                report.append({\"City\": city, \"Original\": len(df_task), \"Filtered\": len(df_filtered), \"Reduction\": f\"{reduction:.1f}%\"})\n",
    "\n",
    "        print(\"\\n--- Filtering Summary (Cleaned POI 2025) ---\")\n",
    "        print(pd.DataFrame(report))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = UrbanDataOptimizer()\n",
    "    # æ‰§è¡Œè¿‡æ»¤\n",
    "    optimizer.filter_active_grids(strict_2025=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import googlemaps\n",
    "from tqdm import tqdm\n",
    "\n",
    "class UrbanAtomicHarvesterPersistent:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– Google Maps å®¢æˆ·ç«¯å¹¶é…ç½®æŒä¹…åŒ–è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.gmaps = googlemaps.Client(key=api_key, timeout=10) \n",
    "        self.home_dir = os.path.expanduser(\"~\")\n",
    "        \n",
    "        self.base_path = os.path.join(self.home_dir, \"Desktop/Infor 301/final\")\n",
    "        self.task_dir = os.path.join(self.base_path, \"cleaned_POI_2025\")\n",
    "        self.output_dir = os.path.join(self.base_path, \"Raw_POI_Archive_2025\")\n",
    "        \n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        self.categories = [\n",
    "            \"bank\", \"jewelry_store\", \"art_gallery\", \"museum\", \n",
    "            \"department_store\", \"shopping_mall\", \"university\", \n",
    "            \"hospital\", \"supermarket\", \"restaurant\",\n",
    "            \"subway_station\", \"park\"\n",
    "        ]\n",
    "\n",
    "    def harvest_grid_data(self, lat, lon):\n",
    "        counts = {cat: 0 for cat in self.categories}\n",
    "        try:\n",
    "            response = self.gmaps.places_nearby(location=(lat, lon), radius=500)\n",
    "            results = response.get('results', [])\n",
    "            \n",
    "            for place in results:\n",
    "                place_types = place.get('types', [])\n",
    "                for cat in self.categories:\n",
    "                    if cat in place_types:\n",
    "                        counts[cat] += 1\n",
    "            return counts\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[Warning] API call failed at ({lat}, {lon}): {e}\")\n",
    "            return counts\n",
    "\n",
    "    def run(self):\n",
    "        task_files = [f for f in os.listdir(self.task_dir) if f.endswith(\".csv\")]\n",
    "        \n",
    "        if not task_files:\n",
    "            print(\"Error: No task files found.\")\n",
    "            return\n",
    "\n",
    "        for file in task_files:\n",
    "            city_name = file.split('_')[0]\n",
    "            save_path = os.path.join(self.output_dir, f\"{city_name}_RAW_2025.csv\")\n",
    "            \n",
    "            df_task = pd.read_csv(os.path.join(self.task_dir, file))\n",
    "            \n",
    "            if os.path.exists(save_path):\n",
    "                try:\n",
    "                    df_existing = pd.read_csv(save_path)\n",
    "                    done_count = len(df_existing)\n",
    "                    \n",
    "                    if done_count >= len(df_task):\n",
    "                        print(f\"\\n[Complete] {city_name} is already finished. Skipping.\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"\\n[Resuming] {city_name} from row {done_count}\")\n",
    "                        results_list = df_existing.to_dict('records')\n",
    "                        df_remaining = df_task.iloc[done_count:]\n",
    "                except Exception:\n",
    "\n",
    "                    results_list = []\n",
    "                    df_remaining = df_task\n",
    "            else:\n",
    "                results_list = []\n",
    "                df_remaining = df_task\n",
    "\n",
    "\n",
    "            for idx, row in tqdm(df_remaining.iterrows(), total=len(df_remaining), desc=f\"Processing {city_name}\"):\n",
    "                atomic_counts = self.harvest_grid_data(row['latitude'], row['longitude'])\n",
    "                \n",
    "                record = row.to_dict()\n",
    "                record.update(atomic_counts)\n",
    "                results_list.append(record)\n",
    "                \n",
    "    \n",
    "                if len(results_list) % 10 == 0:\n",
    "                    pd.DataFrame(results_list).to_csv(save_path, index=False)\n",
    "                \n",
    "  \n",
    "                time.sleep(0.05) \n",
    "\n",
    "\n",
    "            pd.DataFrame(results_list).to_csv(save_path, index=False)\n",
    "            print(f\"\\n[Finished] {city_name} total {len(results_list)} rows saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MY_KEY = \"AIzaSyDr5kPdH5U6s6qltCBpAFO7dxTFibKGFXg\"\n",
    "    \n",
    "    harvester = UrbanAtomicHarvesterPersistent(MY_KEY)\n",
    "    harvester.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
